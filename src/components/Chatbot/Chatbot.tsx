import React, { useState, useRef, useEffect } from 'react';
import styles from './Chatbot.module.css';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  sources?: Array<{
    title: string;
    excerpt: string;
    path: string;
  }>;
}

type ChatbotProps = {
  compact?: boolean;
};

export default function Chatbot({ compact = false }: ChatbotProps): React.ReactElement {
  const [messages, setMessages] = useState<Message[]>([
    {
      role: 'assistant',
      content: 'Welcome! ðŸ‘‹ I\'m your AI & Robotics Textbook Assistant. I can help you understand any concept from this comprehensive textbook covering AI, machine learning, robotics, and more. What would you like to learn about?',
    },
  ]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const getAnswer = (question: string): string => {
    const lowerQuestion = question.toLowerCase();

    // Greetings and creator info
    if (lowerQuestion.match(/^(hi|hello|hey|greetings|namaste|assalam|salaam)/)) {
      return 'Hello! ðŸ‘‹ Welcome to the AI & Robotics Textbook! I\'m your assistant here to help you explore this comprehensive resource on AI, machine learning, robotics, and emerging technologies. Feel free to ask me anything about any topic in the book!';
    }

    if (lowerQuestion.includes('created by') || lowerQuestion.includes('who made') || lowerQuestion.includes('author')) {
      return 'ðŸ“š This is a comprehensive textbook bringing together expertise in AI, robotics, and advanced machine learning technologies. It aims to provide a complete understanding of how AI and robotics intersect and transform each other, covering everything from fundamentals to cutting-edge research directions.';
    }

    // Introduction to AI and Robotics
    if (lowerQuestion.includes('introduction') && (lowerQuestion.includes('ai') || lowerQuestion.includes('robotics') || lowerQuestion.includes('book'))) {
      return 'The Introduction chapter explores the convergence of AI and Robotics:\n\nðŸ¤– AI provides machines with:\nâ€¢ Perception (understanding the world)\nâ€¢ Learning (improving from experience)\nâ€¢ Decision-making (choosing optimal actions)\nâ€¢ Reasoning (understanding complex problems)\n\nðŸ¦¾ Robotics provides:\nâ€¢ Physical embodiment (sensors & actuators)\nâ€¢ Real-time control (fast response)\nâ€¢ Manipulation (moving and changing the world)\nâ€¢ Autonomous operation\n\nTogether, they create intelligent machines that can work in complex, unstructured environments alongside humans.';
    }

    // Machine Learning Fundamentals
    if (lowerQuestion.includes('machine learning') || (lowerQuestion.includes('supervised') && lowerQuestion.includes('learning'))) {
      return 'ðŸ“Š Machine Learning Fundamentals:\n\n**Three Main Paradigms:**\n\n1ï¸âƒ£ Supervised Learning:\nâ€¢ Learn from labeled data (inputâ†’output pairs)\nâ€¢ Classification: predict categories\nâ€¢ Regression: predict continuous values\nâ€¢ Examples: predicting house prices, image classification\n\n2ï¸âƒ£ Unsupervised Learning:\nâ€¢ Find patterns in unlabeled data\nâ€¢ Clustering: group similar items\nâ€¢ Dimensionality reduction: simplify high-dimensional data\nâ€¢ Examples: customer segmentation, data visualization\n\n3ï¸âƒ£ Reinforcement Learning:\nâ€¢ Learn through trial and error with rewards/penalties\nâ€¢ Agent interacts with environment\nâ€¢ Goal: maximize cumulative reward\nâ€¢ Examples: game playing, robot control\n\nKey concept: Choose the right algorithm for your problem!';
    }

    // Computer Vision
    if (lowerQuestion.includes('computer vision') || (lowerQuestion.includes('image') && (lowerQuestion.includes('recognition') || lowerQuestion.includes('detection') || lowerQuestion.includes('segmentation')))) {
      return 'ðŸ‘ï¸ Computer Vision Overview:\n\nComputer vision enables machines to interpret visual information.\n\n**Key Tasks:**\n\nðŸ–¼ï¸ Image Classification: Classify entire image (dog vs cat)\nðŸŽ¯ Object Detection: Find and locate objects in image\nðŸ“ Semantic Segmentation: Classify each pixel\nðŸ‘¤ Instance Segmentation: Identify individual objects\n3ï¸âƒ£ 3D Vision: Understand depth and 3D structure\n\n**Main Architecture:**\nðŸ§  Convolutional Neural Networks (CNNs):\nâ€¢ Convolutional layers: Extract features using filters\nâ€¢ Pooling layers: Downsample and reduce parameters\nâ€¢ Fully connected layers: Make final predictions\nâ€¢ Each layer learns hierarchical features\n\n**Applications in Robotics:**\nâ€¢ Robot vision for object picking\nâ€¢ Autonomous vehicle perception\nâ€¢ Gesture recognition\nâ€¢ Scene understanding';
    }

    // Robot Control
    if (lowerQuestion.includes('robot control') || lowerQuestion.includes('pid') || (lowerQuestion.includes('control') && (lowerQuestion.includes('trajectory') || lowerQuestion.includes('kinematics')))) {
      return 'âš™ï¸ Robot Control Systems:\n\n**Control Objectives:**\n1. Make robot move to desired position\n2. Follow planned trajectories\n3. Maintain stability\n4. Handle disturbances\n\n**Control Methods:**\n\nðŸ“Œ PID Control (Classic):\nâ€¢ Proportional: React to current error\nâ€¢ Integral: Correct persistent error\nâ€¢ Derivative: Anticipate future error\nâ€¢ Used for individual joint control\n\nðŸ›¤ï¸ Trajectory Planning:\nâ€¢ Generate smooth paths in configuration space\nâ€¢ Avoid singularities and joint limits\nâ€¢ Minimize time, energy, or smoothness\nâ€¢ Result: sequence of waypoints\n\nðŸ”„ Inverse Kinematics:\nâ€¢ Convert desired end-effector position to joint angles\nâ€¢ Multiple solutions possible (redundancy)\nâ€¢ Numerical vs analytical solutions\nâ€¢ Essential for precise manipulation\n\nðŸ§  Learning-Based Control:\nâ€¢ Neural networks learn control policies\nâ€¢ Adapt to changing environments\nâ€¢ More robust to uncertainties\nâ€¢ Requires training data/simulation';
    }

    // Navigation
    if (lowerQuestion.includes('navigation') || lowerQuestion.includes('slam') || lowerQuestion.includes('path planning') || lowerQuestion.includes('localization')) {
      return 'ðŸ—ºï¸ Robot Navigation:\n\n**Three Core Components:**\n\nðŸ§­ Localization:\nâ€¢ Where am I? (robot position & orientation)\nâ€¢ GPS works outdoors\nâ€¢ IMU, odometry for relative movement\nâ€¢ Must handle accumulated error\n\nðŸ“ Mapping:\nâ€¢ What\'s around me?\nâ€¢ Build 2D/3D maps of environment\nâ€¢ Sensor fusion from multiple sensors\nâ€¢ Occupancy grids, feature maps\n\nðŸŽ¯ Path Planning:\nâ€¢ How do I reach goal?\nâ€¢ A*: Optimal path with heuristics\nâ€¢ RRT: Rapidly-exploring Random Trees\nâ€¢ Dijkstra, Bellman-Ford algorithms\n\n**SLAM (Simultaneous Localization and Mapping):**\nâ€¢ Solve chicken-and-egg problem\nâ€¢ Build map while figuring out location\nâ€¢ Essential for autonomous robots\nâ€¢ FastSLAM, Graph-SLAM popular approaches\n\n**Modern Approaches:**\nâ€¢ End-to-end learning with neural networks\nâ€¢ Visual inertial odometry\nâ€¢ LiDAR-based approaches';
    }

    // Deep Learning
    if (lowerQuestion.includes('deep learning') || lowerQuestion.includes('neural network') || lowerQuestion.includes('cnn') || lowerQuestion.includes('lstm') || lowerQuestion.includes('transformer')) {
      return 'ðŸ§  Deep Learning:\n\n**Why Deep?**\nâ€¢ Hierarchical representations\nâ€¢ Lower layers: simple features (edges, corners)\nâ€¢ Higher layers: complex concepts (objects, scenes)\nâ€¢ More layers = more expressive power\n\n**Key Architectures:**\n\nðŸŽ¨ Convolutional Neural Networks (CNNs):\nâ€¢ For images and spatial data\nâ€¢ Filters detect local patterns\nâ€¢ Pooling reduces dimensionality\nâ€¢ Architecture: Conv â†’ ReLU â†’ Pool â†’ FC\n\nðŸ“ˆ Recurrent Neural Networks (RNNs):\nâ€¢ For sequences and time series\nâ€¢ Hidden state carries information forward\nâ€¢ Process one element at a time\nâ€¢ Problem: Vanishing gradients over long sequences\n\nðŸ”— LSTM (Long Short-Term Memory):\nâ€¢ Solution to RNN gradient problems\nâ€¢ Memory cells store long-term info\nâ€¢ Gates control information flow\nâ€¢ Can learn 100+ step dependencies\n\nâš¡ Transformers:\nâ€¢ Process sequences in parallel (not sequential)\nâ€¢ Attention mechanism: focus on relevant parts\nâ€¢ Much faster training than RNNs\nâ€¢ Foundation for modern LLMs\nâ€¢ Multi-head attention: multiple perspectives\n\n**Training Deep Networks:**\nâ€¢ Backpropagation: compute gradients efficiently\nâ€¢ SGD, Adam: optimization algorithms\nâ€¢ Dropout, batch norm: regularization\nâ€¢ GPU/TPU: essential for large-scale training';
    }

    // NLP
    if (lowerQuestion.includes('nlp') || lowerQuestion.includes('natural language') || lowerQuestion.includes('bert') || lowerQuestion.includes('gpt') || lowerQuestion.includes('language model')) {
      return 'ðŸ“ Natural Language Processing:\n\n**Text Representation:**\n\nðŸ“Š Bag of Words:\nâ€¢ Simple but loses word order\nâ€¢ Count occurrences of each word\nâ€¢ Works for some tasks\n\nðŸ”¢ Word Embeddings:\nâ€¢ Represent words as dense vectors\nâ€¢ Similar words have similar vectors\nâ€¢ Word2Vec, GloVe, FastText\nâ€¢ Enable semantic operations: king - man + woman â‰ˆ queen\n\n**Key Models:**\n\nðŸ”¤ BERT (Bidirectional Encoder):\nâ€¢ Pre-trained on massive text corpus\nâ€¢ Understands context from both directions\nâ€¢ Fine-tune for: classification, NER, Q&A\nâ€¢ Foundation for many NLP applications\n\nðŸ“¢ GPT (Generative Pre-trained Transformer):\nâ€¢ Autoregressive: predicts next word\nâ€¢ Excellent for text generation\nâ€¢ Few-shot learning capability\nâ€¢ Powers ChatGPT, modern LLMs\n\n**Tasks:**\n\nðŸ˜Š Sentiment Analysis: Positive/negative/neutral\nðŸ·ï¸ Named Entity Recognition: Find people, places, organizations\nðŸŒ Machine Translation: Translate between languages\nâ“ Question Answering: Find answers in documents\nðŸ’¬ Dialogue: Conversational systems\nðŸ“° Text Summarization: Condense long text\n\n**Modern Approach:**\nâ€¢ Pre-train on huge corpus\nâ€¢ Fine-tune on specific task\nâ€¢ Transfer learning is key\nâ€¢ Transformers dominate the field';
    }

    // Reinforcement Learning
    if (lowerQuestion.includes('reinforcement learning') || lowerQuestion.includes('q-learning') || lowerQuestion.includes('reward') || (lowerQuestion.includes('policy') && lowerQuestion.includes('learning'))) {
      return 'ðŸŽ® Reinforcement Learning (RL):\n\n**Core Concept:**\nAgent learns optimal behavior through interaction:\nâ€¢ Takes action in environment\nâ€¢ Receives reward/penalty\nâ€¢ Updates its policy\nâ€¢ Goal: Maximize cumulative reward\n\n**Key Components:**\n\nðŸŒ Environment:\nâ€¢ State: current situation\nâ€¢ Action: choices available\nâ€¢ Transition: state probability given action\nâ€¢ Reward: feedback signal\n\nðŸ¤– Agent:\nâ€¢ Policy: mapping state â†’ action\nâ€¢ Value function: expected reward from state\nâ€¢ Action-value: expected reward from action in state\n\n**Popular Algorithms:**\n\nðŸ“Œ Q-Learning:\nâ€¢ Learn state-action values (Q-values)\nâ€¢ Off-policy: learn from any experience\nâ€¢ Bellman equation: Q(s,a) = r + Î³ max Q(s\',a\')\nâ€¢ Converges to optimal policy\nâ€¢ Issue: Table too large for complex problems\n\nðŸ§  Deep Q-Networks (DQN):\nâ€¢ Use neural network to approximate Q-values\nâ€¢ Experience replay: store and sample memories\nâ€¢ Target network: separate network for stability\nâ€¢ Breakthrough: Learned Atari games from pixels\n\nðŸ“ˆ Policy Gradient:\nâ€¢ Directly optimize policy parameters\nâ€¢ Gradient ascent on expected reward\nâ€¢ Handles continuous action spaces\nâ€¢ Examples: REINFORCE, Actor-Critic\n\n**Applications in Robotics:**\nâ€¢ Robot manipulation and grasping\nâ€¢ Locomotion and walking\nâ€¢ Navigation and obstacle avoidance\nâ€¢ Game playing and control tasks';
    }

    // Humanoid Robotics
    if (lowerQuestion.includes('humanoid') || lowerQuestion.includes('bipedal') || (lowerQuestion.includes('robot') && (lowerQuestion.includes('grasping') || lowerQuestion.includes('manipulation') || lowerQuestion.includes('arm')))) {
      return 'ðŸ¦¾ Humanoid Robotics:\n\n**Why Humanoid Form?**\nâ€¢ Designed for human environments\nâ€¢ Can use human tools and spaces\nâ€¢ Natural interaction with humans\nâ€¢ Familiar movement patterns\n\n**Key Challenges:**\n\nâš–ï¸ Bipedal Balance:\nâ€¢ 2 contact points (unstable)\nâ€¢ Zero Moment Point (ZMP) must stay in support polygon\nâ€¢ Dynamic balance vs static balance\nâ€¢ Active balance control essential\n\nðŸ¦¾ Dexterous Manipulation:\nâ€¢ Multi-fingered hands (typically 5 fingers)\nâ€¢ High DOF (degrees of freedom)\nâ€¢ Complex coordination required\nâ€¢ In-hand manipulation difficult\n\nðŸ‘ï¸ Perception:\nâ€¢ Cameras, IMU, force sensors\nâ€¢ Real-time processing required\nâ€¢ Environment must be understood\nâ€¢ Sensor fusion essential\n\n**Design Elements:**\n\nðŸ§  Actuation:\nâ€¢ Electric motors: precise, clean\nâ€¢ Pneumatic: powerful, compliant\nâ€¢ Hydraulic: very strong but slow\nâ€¢ Series elastic: absorb impacts safely\n\nðŸ“ Kinematics:\nâ€¢ Forward kinematics: joint angles â†’ end-effector pose\nâ€¢ Inverse kinematics: desired pose â†’ joint angles\nâ€¢ Redundancy: more DOF than necessary\nâ€¢ Workspace: reachable positions\n\n**State-of-the-art Systems:**\nâ€¢ Tesla Optimus: approaching mass production\nâ€¢ Boston Dynamics Atlas: acrobatic capabilities\nâ€¢ Sanctuary AI Phoenix: advanced dexterity\nâ€¢ Honda Asimo: social interaction pioneer\n\n**Control Strategies:**\nâ€¢ Hierarchical: task â†’ motion â†’ joint level\nâ€¢ Whole-body: coordinate all joints\nâ€¢ Learning from demonstration\nâ€¢ Reinforcement learning for adaptation';
    }

    // Knowledge Systems
    if (lowerQuestion.includes('knowledge') || lowerQuestion.includes('ontology') || lowerQuestion.includes('semantic') || lowerQuestion.includes('neurosymbolic')) {
      return 'ðŸ§  Knowledge Systems and Reasoning:\n\n**Traditional AI vs Modern Learning:**\n\nðŸ“š Symbolic AI:\nâ€¢ Explicit knowledge representation\nâ€¢ Rules: IF condition THEN action\nâ€¢ Logical reasoning: derive new facts\nâ€¢ Explainable and interpretable\nâ€¢ Limited to pre-programmed knowledge\n\nðŸ§  Neural AI:\nâ€¢ Learn from data automatically\nâ€¢ Patterns in high-dimensional spaces\nâ€¢ Black box: hard to explain\nâ€¢ Requires large amounts of data\nâ€¢ Robust to noise and variations\n\n**Knowledge Representation:**\n\nðŸŒ Knowledge Graphs:\nâ€¢ Nodes: entities (objects, concepts)\nâ€¢ Edges: relationships\nâ€¢ Semantic meaning captured\nâ€¢ Examples: Google Knowledge Graph, DBpedia\n\nðŸ“‹ Ontologies:\nâ€¢ Formal definitions of domain concepts\nâ€¢ Class hierarchies: Dog is-a Animal\nâ€¢ Properties and relationships\nâ€¢ Enable semantic web technologies\n\nðŸ”— Semantic Networks:\nâ€¢ Concepts connected by relations\nâ€¢ Spreading activation for inference\nâ€¢ Simple but powerful\n\n**Neurosymbolic AI:**\nðŸ’¡ Combines best of both worlds:\nâ€¢ Neural networks: learn patterns from data\nâ€¢ Symbolic systems: reason over representations\nâ€¢ Knowledge-guided learning: incorporate constraints\nâ€¢ Better generalization and interpretability\n\n**Applications:**\nâ€¢ Robotics: ground language in perception\nâ€¢ Knowledge extraction from text\nâ€¢ Hybrid reasoning systems\nâ€¢ AI safety and verification';
    }

    // Emerging Topics
    if (lowerQuestion.includes('emerging') || lowerQuestion.includes('future') || lowerQuestion.includes('meta-learning') || lowerQuestion.includes('multimodal') || lowerQuestion.includes('vision transformer') || lowerQuestion.includes('nerf')) {
      return 'ðŸš€ Emerging Topics and Future Directions:\n\n**Few-Shot Learning:**\nâ€¢ Learn from minimal examples (1-5 shots)\nâ€¢ Meta-learning: learn how to learn\nâ€¢ Quick adaptation to new tasks\nâ€¢ Prototypical networks, matching networks\n\n**Zero-Shot Learning:**\nâ€¢ Learn without task-specific examples\nâ€¢ Transfer knowledge from related tasks\nâ€¢ Semantic descriptions of new classes\nâ€¢ Enables learning of unseen categories\n\n**Vision Transformers:**\nâ€¢ Apply transformer architecture to images\nâ€¢ Split image into patches\nâ€¢ Self-attention between patches\nâ€¢ Outperforms CNNs on large datasets\nâ€¢ Stronger transfer learning\n\n**Multimodal Models:**\nðŸ–¼ï¸ Vision + Language: GPT-4V, CLIP, BLIP\nâ€¢ Understand images with text descriptions\nâ€¢ Powerful for reasoning about visual content\n\nðŸŽ™ï¸ Audio + Language: Whisper\nâ€¢ Speech-to-text, transcription\nâ€¢ Robust to accents and noise\n\nðŸ¤– Vision + Language + Action: Robotics foundation models\nâ€¢ Learn robot control from videos\nâ€¢ Generalize to new tasks\n\n**Neural Radiance Fields (NeRF):**\nâ€¢ Implicit 3D representation\nâ€¢ Novel view synthesis\nâ€¢ 3D reconstruction from images\nâ€¢ Real-time rendering advances\n\n**Key Challenges:**\nâš ï¸ Robustness: Adversarial examples, distribution shift\nðŸ“Š Data efficiency: Learn from few examples\nðŸ›¡ï¸ Safety: Ensure AI behaves as intended\nðŸŽ¯ Interpretability: Understand model decisions\nâš¡ Efficiency: Deploy on edge devices\nðŸŒ Generalization: Work in new environments\n\n**Future Research Directions:**\nâ€¢ Continual learning: adapt without forgetting\nâ€¢ Embodied AI: learn through interaction\nâ€¢ AI safety and alignment\nâ€¢ Energy-efficient models\nâ€¢ Brain-inspired computing\nâ€¢ Quantum machine learning';
    }

    // Future Chapter
    if (lowerQuestion.includes('future') || lowerQuestion.includes('next') || lowerQuestion.includes('what\'s coming')) {
      return 'ðŸ”® The Future of AI and Robotics:\n\n**Near-term (1-5 years):**\nâ€¢ Humanoid robots in manufacturing\nâ€¢ Autonomous delivery systems\nâ€¢ Advanced medical robotics\nâ€¢ More capable LLMs\nâ€¢ Better sim-to-real transfer\n\n**Medium-term (5-10 years):**\nâ€¢ Home assistant robots\nâ€¢ Fully autonomous vehicles\nâ€¢ AI-augmented drug discovery\nâ€¢ Brain-computer interfaces\nâ€¢ General-purpose embodied agents\n\n**Challenges to Overcome:**\nâ“ Generalization: Work in new environments\nðŸ’° Cost: Make systems affordable\nðŸ”’ Safety: Ensure reliable operation\nâš–ï¸ Ethics: Fair and beneficial AI\nðŸŒ Environmental: Sustainable deployment\nðŸ¤ Human-AI: Better collaboration\n\n**Opportunities:**\nâœ¨ Healthcare: Personalized medicine, surgery\nðŸŒ± Environment: Climate modeling, resource management\nðŸŽ“ Education: Personalized learning\nðŸ­ Industry: Flexible automation\nðŸš€ Space: Autonomous exploration\n\n**The Bottom Line:**\nAI and robotics will transform society. Success depends on:\nâ€¢ Technical innovation\nâ€¢ Ethical development\nâ€¢ Inclusive deployment\nâ€¢ Continuous learning and adaptation';
    }

    // Default/general response
    return 'ðŸ“š Great question! This comprehensive textbook covers 12 chapters:\n\n1. Introduction to AI & Robotics\n2. Machine Learning Fundamentals\n3. Computer Vision\n4. Robot Control\n5. Navigation & Autonomous Systems\n6. Deep Learning\n7. Natural Language Processing\n8. Reinforcement Learning\n9. Humanoid Robotics\n10. Knowledge Systems\n11. Emerging Topics\n12. Future of AI & Robotics\n\nTry asking about any specific chapter or topic! Examples:\nâ€¢ "Tell me about deep learning"\nâ€¢ "How does computer vision work?"\nâ€¢ "What is reinforcement learning?"\nâ€¢ "Tell me about humanoid robots"\nâ€¢ "Who created this textbook?"\n\nWhat interests you most?';
  };

  const handleSend = async () => {
    if (!input.trim()) return;

    const userMessage: Message = { role: 'user', content: input };
    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);

    // Call backend RAG endpoint; fall back to local rule-based answer on error
    try {
      const resp = await fetch('http://localhost:8000/api/v1/rag', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ query: input }),
      });

      if (resp.ok) {
        const data = await resp.json();
        const assistantMessage: Message = {
          role: 'assistant',
          content: data.answer || getAnswer(input),
          sources: data.sources || [],
        };
        setMessages(prev => [...prev, assistantMessage]);
      } else {
        const answer = getAnswer(input);
        const assistantMessage: Message = { role: 'assistant', content: answer };
        setMessages(prev => [...prev, assistantMessage]);
      }
    } catch (err) {
      const answer = getAnswer(input);
      const assistantMessage: Message = { role: 'assistant', content: answer };
      setMessages(prev => [...prev, assistantMessage]);
    } finally {
      setIsLoading(false);
    }
  };

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSend();
    }
  };

  return (
    <div
      className={styles.chatbotContainer}
      style={{ height: compact ? 360 : undefined, maxWidth: compact ? 360 : undefined }}
    >
      <div className={styles.chatbotHeader}>
        <h3>ðŸ“š Textbook Assistant</h3>
      </div>
      <div className={styles.messagesContainer}>
        {messages.map((msg, idx) => (
          <div key={idx} className={`${styles.message} ${styles[msg.role]}`}>
            <div className={styles.messageContent}>
              <div>{msg.content}</div>

              {msg.sources && msg.sources.length > 0 && (
                <div className={styles.sources}>
                  <div className={styles.sourceList}>
                    {msg.sources.map((s, i) => (
                      <div key={i} className={styles.sourceItem}>
                        <span className={styles.sourceTitle}>{s.title}</span>
                        <span className={styles.sourceExcerpt}>{s.excerpt}</span>
                        {s.path && (
                          <a className={styles.sourceLink} href={`/docs/${s.path.replace(/\.md$/,'')}`} target="_blank" rel="noreferrer">Open</a>
                        )}
                      </div>
                    ))}
                  </div>
                </div>
              )}

            </div>
          </div>
        ))}
        {isLoading && (
          <div className={`${styles.message} ${styles.assistant}`}>
            <div className={styles.messageContent}>
              <span className={styles.typing}>Thinking...</span>
            </div>
          </div>
        )}
        <div ref={messagesEndRef} />
      </div>
      <div className={styles.inputContainer}>
        <textarea
          value={input}
          onChange={e => setInput(e.target.value)}
          onKeyPress={handleKeyPress}
          placeholder="Ask me about AI & robotics"
          className={styles.input}
          disabled={isLoading}
          rows={2}
        />
        <button
          onClick={handleSend}
          disabled={isLoading || !input.trim()}
          className={styles.sendButton}
        >
          Send
        </button>
      </div>
    </div>
  );
}
